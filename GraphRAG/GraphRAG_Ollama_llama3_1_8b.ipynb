{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrJWIHM28BMbHcxWtC+RgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ningxia202109/llm-learn/blob/main/GraphRAG/GraphRAG_Ollama_llama3_1_8b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eLPtin9qD9S"
      },
      "outputs": [],
      "source": [
        "# Ollama on T4 GPU\n",
        "# Install Ollama and GPU Package\n",
        "%%capture --no-stderr\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "os.environ.update({'OLLAMA_HOST': '0.0.0.0'})\n",
        "\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "\n",
        "def iframe_thread(port):\n",
        "    while True:\n",
        "        time.sleep(0.5)\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        result = sock.connect_ex(('127.0.0.1', port))\n",
        "        if result == 0:\n",
        "            break\n",
        "        sock.close()\n",
        "\n",
        "    p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{port}\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for line in p.stderr:\n",
        "        l = line.decode()\n",
        "        if \"trycloudflare.com \" in l:\n",
        "            print(\"\\n\\n\\n\\n\\n\")\n",
        "            print(\"running ollama server\\n\\n\", l[l.find(\"http\"):], end='')\n",
        "            print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(11434,)).start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Ollama\n",
        "MODEL_NAME=\"llama3.1:8b\"\n",
        "!ollama serve > ollama-server.log 2>&1 &\n",
        "!ollama --version\n",
        "\n",
        "# Install LLM model\n",
        "!ollama run llama3.1:8b > llama3-1-8b.log 2>&1 &\n",
        "# Wait for AI MODEL\n",
        "!while ! ollama list | grep -q \"$MODEL_NAME\"; do \\\n",
        "  echo \"Waiting for $MODEL_NAME to become available...\"; \\\n",
        "  sleep 15; \\\n",
        "done\n",
        "!echo \"$MODEL_NAME is now available.\"\n",
        "\n",
        "# Install Ollama embedding\n",
        "!ollama pull mxbai-embed-large\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "JaEnLuVGq2za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages of GraphRAG\n",
        "%%capture --no-stderr\n",
        "!pip install graphrag"
      ],
      "metadata": {
        "id": "pGGPIhEarvGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intial GraphRAG folder\n",
        "\n",
        "%cd /content\n",
        "!mkdir -p ./ragtest/input\n",
        "!python -m graphrag.index --init --root ./ragtest"
      ],
      "metadata": {
        "id": "Fq58cAo7slr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurea GraphRAG\n",
        "%%writefile ./ragtest/settings.yaml\n",
        "\n",
        "encoding_model: cl100k_base\n",
        "skip_workflows: []\n",
        "llm:\n",
        "  api_key: ollama\n",
        "  type: openai_chat # or azure_openai_chat\n",
        "  model:  llama3.1:8b\n",
        "  model_supports_json: true # recommended if this is available for your model.\n",
        "  max_tokens: 3000\n",
        "  # request_timeout: 180.0\n",
        "  api_base: http://localhost:11434/v1\n",
        "  # api_version: 2024-02-15-preview\n",
        "  # organization:\n",
        "  # deployment_name:\n",
        "  tokens_per_minute: 6000 # set a leaky bucket throttle\n",
        "  requests_per_minute: 2 # set a leaky bucket throttle\n",
        "  max_retries: 3\n",
        "  # max_retry_wait: 10.0\n",
        "  # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n",
        "  # concurrent_requests: 25 # the number of parallel inflight requests that may be made\n",
        "\n",
        "parallelization:\n",
        "  stagger: 0.3\n",
        "  # num_threads: 50 # the number of threads to use for parallel processing\n",
        "\n",
        "async_mode: threaded # or asyncio\n",
        "\n",
        "embeddings:\n",
        "  ## parallelization: override the global parallelization settings for embeddings\n",
        "  async_mode: threaded # or asyncio\n",
        "  llm:\n",
        "    api_key: ollama\n",
        "    # type: openai_embedding # or azure_openai_embedding\n",
        "    model: mxbai-embed-large\n",
        "    api_base: http://localhost:11434/api/embeddings\n",
        "    # api_version: 2024-02-15-preview\n",
        "    # organization:\n",
        "    # deployment_name:\n",
        "    # tokens_per_minute: 150_000 # set a leaky bucket throttle\n",
        "    # requests_per_minute: 10_000 # set a leaky bucket throttle\n",
        "    # max_retries: 10\n",
        "    # max_retry_wait: 10.0\n",
        "    # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n",
        "    # concurrent_requests: 25 # the number of parallel inflight requests that may be made\n",
        "    # batch_size: 16 # the number of documents to send in a single request\n",
        "    # batch_max_tokens: 8191 # the maximum number of tokens to send in a single request\n",
        "    # target: required # or optional\n",
        "\n",
        "\n",
        "\n",
        "chunks:\n",
        "  size: 300\n",
        "  overlap: 100\n",
        "  group_by_columns: [id] # by default, we don't allow chunks to cross documents\n",
        "\n",
        "input:\n",
        "  type: file # or blob\n",
        "  file_type: text # or csv\n",
        "  base_dir: \"input\"\n",
        "  file_encoding: utf-8\n",
        "  file_pattern: \".*\\.txt$\"\n",
        "\n",
        "cache:\n",
        "  type: file # or blob\n",
        "  base_dir: \"cache\"\n",
        "  # connection_string:\n",
        "  # container_name:\n",
        "\n",
        "storage:\n",
        "  type: file # or blob\n",
        "  base_dir: \"output/${timestamp}/artifacts\"\n",
        "  # connection_string:\n",
        "  # container_name:\n",
        "\n",
        "reporting:\n",
        "  type: file # or console, blob\n",
        "  base_dir: \"output/${timestamp}/reports\"\n",
        "  # connection_string:\n",
        "  # container_name:\n",
        "\n",
        "entity_extraction:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/entity_extraction.txt\"\n",
        "  entity_types: [organization,person,geo,event]\n",
        "  max_gleanings: 0\n",
        "\n",
        "summarize_descriptions:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/summarize_descriptions.txt\"\n",
        "  max_length: 500\n",
        "\n",
        "claim_extraction:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  # enabled: true\n",
        "  prompt: \"prompts/claim_extraction.txt\"\n",
        "  description: \"Any claims or facts that could be relevant to information discovery.\"\n",
        "  max_gleanings: 0\n",
        "\n",
        "community_report:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/community_report.txt\"\n",
        "  max_length: 2000\n",
        "  max_input_length: 8000\n",
        "\n",
        "cluster_graph:\n",
        "  max_cluster_size: 10\n",
        "\n",
        "embed_graph:\n",
        "  enabled: false # if true, will generate node2vec embeddings for nodes\n",
        "  # num_walks: 10\n",
        "  # walk_length: 40\n",
        "  # window_size: 2\n",
        "  # iterations: 3\n",
        "  # random_seed: 597832\n",
        "\n",
        "umap:\n",
        "  enabled: false # if true, will generate UMAP embeddings for nodes\n",
        "\n",
        "snapshots:\n",
        "  graphml: true # Genarate graphml for viewing\n",
        "  raw_entities: false\n",
        "  top_level_nodes: false\n",
        "\n",
        "local_search:\n",
        "  # text_unit_prop: 0.5\n",
        "  # community_prop: 0.1\n",
        "  # conversation_history_max_turns: 5\n",
        "  # top_k_mapped_entities: 10\n",
        "  # top_k_relationships: 10\n",
        "  # max_tokens: 12000\n",
        "\n",
        "global_search:\n",
        "  # max_tokens: 12000\n",
        "  # data_max_tokens: 12000\n",
        "  # map_max_tokens: 1000\n",
        "  # reduce_max_tokens: 2000\n",
        "  # concurrency: 32\n",
        "\n"
      ],
      "metadata": {
        "id": "zjwIIndSs6Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "%%writefile /content/ragtest/input/sample.txt\n",
        "\n",
        "将进酒\n",
        "君不见黄河之水天上来，奔流到海不复回。\n",
        "君不见高堂明镜悲白发，朝如青丝暮成雪。\n",
        "人生得意须尽欢，莫使金樽空对月。\n",
        "天生我材必有用，千金散尽还复来。\n",
        "烹羊宰牛且为乐，会须一饮三百杯。\n",
        "岑夫子，丹丘生，将进酒，杯莫停。\n",
        "与君歌一曲，请君为我倾耳听。(倾耳听 一作：侧耳听)\n",
        "钟鼓馔玉不足贵，但愿长醉不愿醒。(不足贵 一作：何足贵；不愿醒 一作：不复醒)\n",
        "古来圣贤皆寂寞，惟有饮者留其名。(古来 一作：自古；惟 通：唯)\n",
        "陈王昔时宴平乐，斗酒十千恣欢谑。\n",
        "主人何为言少钱，径须沽取对君酌。\n",
        "五花马、千金裘，呼儿将出换美酒，与尔同销万古愁。"
      ],
      "metadata": {
        "id": "VvpQBzOCwNSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GraphRAG Indexing\n",
        "!python -m graphrag.index --root ./ragtest"
      ],
      "metadata": {
        "id": "RhpGz77fw3Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global query\n",
        "!python -m graphrag.query --root ./ragtest --method global \"将进酒是什么?\""
      ],
      "metadata": {
        "id": "nJWyI8gVxB1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local query\n",
        "!python -m graphrag.query --root ./ragtest --method local \"将进酒是什么?\""
      ],
      "metadata": {
        "id": "VvwgburwxNeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}