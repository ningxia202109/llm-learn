{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJNwne4W1vGfljEVWIhsiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ningxia202109/llm-learn/blob/main/python-langchain-tutorials/langchain_react_play_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play LangChain React following below link\n",
        "https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/"
      ],
      "metadata": {
        "id": "oT1IpOxOD-Ra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KT5aiY29D22w",
        "outputId": "8ea68a4c-a8df-4619-dd85-75e46ccd73c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m424.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install -U -q langchain\n",
        "!pip install -U -q langchain_google_genai\n",
        "!pip install -U -q langchain_community\n",
        "!pip install -U -q langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANG_SMITH_KEY')\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "EeTR3gOdEYOv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [TavilySearchResults(max_results=1)]\n",
        "# Pull the prompt from langchain hub to use\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "nTEtRC4GFYm-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-I4e3bXKB6T",
        "outputId": "ab92096e-8cde-42d4-d6fe-590e8f915fc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: tavily_search_results_json\n",
            "Action Input: what is LangChain?\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.ibm.com/topics/langchain', 'content': 'Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\nTrain, validate, tune and deploy generative AI, foundation models and machine learning capabilities with ease and build AI applications in a fraction of the time with a fraction of the data.\\n When building a chain for an agent, inputs include:\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0(link resides outside ibm.com) are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Launched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI more accessible to enthusiasts in the wake of its widespread popularity.\\n Reimagine how you work with\\xa0AI: our diverse, global team of more than 20,000 AI\\xa0experts can help you quickly and confidently\\xa0design and scale AI and automation across your\\xa0business, working across our own IBM\\xa0watsonx\\xa0technology\\xa0and an open ecosystem of partners to deliver any\\xa0AI model, on any cloud, guided by ethics and trust.\\n While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience.'}]\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: LangChain is a set of tools that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is LangChain?',\n",
              " 'output': 'LangChain is a set of tools that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide.'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the prompt to use - you can modify this!\n",
        "promptReactChat = hub.pull(\"hwchase17/react-chat\")\n",
        "# Construct the ReAct agent\n",
        "agentReactChat = create_react_agent(llm, tools, prompt)\n",
        "agent_executor_react_chat = AgentExecutor(agent=agentReactChat, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "HLp2sDGoMJQm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"what's my name? Only use a tool if needed, otherwise respond with Final Answer\",\n",
        "        # Notice that chat_history is a string, since this prompt is aimed at LLMs, not chat models\n",
        "        \"chat_history\": \"Human: Hi! My name is Bob\\nAI: Hello Bob! Nice to meet you\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-1gLRbqMe2H",
        "outputId": "8a868361-92eb-4074-9d03-80d4d3c7edbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: I cannot answer this question because I do not have access to any personal information.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"what's my name? Only use a tool if needed, otherwise respond with Final Answer\",\n",
              " 'chat_history': 'Human: Hi! My name is Bob\\nAI: Hello Bob! Nice to meet you',\n",
              " 'output': 'I cannot answer this question because I do not have access to any personal information.'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}